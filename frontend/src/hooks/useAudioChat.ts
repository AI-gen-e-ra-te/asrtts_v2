import { useState, useRef, useEffect, useCallback } from 'react';
import { ChatMessage, AppStatus, ServerMessage } from '@/types';
import { translations } from '@/locales';

interface UseAudioChatProps {
  t: typeof translations['en'];
}

export const useAudioChat = ({ t }: UseAudioChatProps) => {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [status, setStatus] = useState<AppStatus>('idle');
  const [isConnected, setIsConnected] = useState(false);

  const websocketRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioQueueRef = useRef<string[]>([]);
  const isPlayingRef = useRef(false);
  const sendQueueRef = useRef<Promise<void>>(Promise.resolve());

  // Blob to Base64 helper
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to convert blob to base64'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };

  // å¤„ç†æœåŠ¡å™¨æ¶ˆæ¯
  const handleServerMessage = (data: ServerMessage) => {
    switch (data.type) {
      case 'user-message':
        // æ›´æ–°ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ›¿æ¢å ä½ç¬¦æˆ–æ·»åŠ æ–°æ¶ˆæ¯ï¼‰
        setMessages((prev) => {
          const lastMsg = prev[prev.length - 1];
          if (lastMsg && lastMsg.role === 'user' && lastMsg.text === t.welcome.user_placeholder) {
            // æ›¿æ¢å ä½ç¬¦
            return [
              ...prev.slice(0, -1),
              { ...lastMsg, text: data.content }
            ];
          }
          // æ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯
          return [...prev, { role: 'user', text: data.content }];
        });
        break;

      case 'text-update':
        // AI æ–‡æœ¬æµå¼æ›´æ–°
        setMessages((prev) => {
          const lastMsg = prev[prev.length - 1];
          if (lastMsg && lastMsg.role === 'ai') {
            return [
              ...prev.slice(0, -1),
              { ...lastMsg, text: lastMsg.text + data.content }
            ];
          }
          return [...prev, { role: 'ai', text: data.content }];
        });
        break;

      case 'audio-chunk':
        audioQueueRef.current.push(data.content);
        playAudioQueue();
        break;

      case 'status':
        if (data.content === 'idle' && audioQueueRef.current.length > 0) {
            // é˜Ÿåˆ—æœªæ’­å®Œï¼Œæš‚ä¸è®¾ç½® idleï¼Œç”±æ’­æ”¾å™¨æ§åˆ¶
        } else {
            setStatus(data.content as AppStatus);
        }
        break;
    }
  };

  // åˆå§‹åŒ– WebSocket
  useEffect(() => {
    // ä»ç¯å¢ƒå˜é‡è¯»å–WebSocket URLï¼Œé»˜è®¤ä¸ºlocalhost:8000
    const wsUrl = process.env.NEXT_PUBLIC_WS_URL || 'ws://localhost:8000/ws/chat';
    const ws = new WebSocket(wsUrl);

    ws.onopen = () => {
      console.log('ğŸ”— Connected to Auralis Backend');
      setIsConnected(true);
    };

    ws.onclose = () => {
      console.log('ğŸ”Œ Disconnected');
      setIsConnected(false);
    };

    ws.onmessage = async (event) => {
      try {
        const data: ServerMessage = JSON.parse(event.data);
        handleServerMessage(data);
      } catch (e) {
        console.error('Failed to parse WebSocket message:', e);
      }
    };

    websocketRef.current = ws;

    return () => {
      ws.close();
    };
  }, []);


  // æ’­æ”¾é˜Ÿåˆ—
  const playAudioQueue = useCallback(async () => {
    if (isPlayingRef.current || audioQueueRef.current.length === 0) return;

    isPlayingRef.current = true;
    setStatus('playing');

    const chunk = audioQueueRef.current.shift();
    if (!chunk) return;

    const audio = new Audio(`data:audio/wav;base64,${chunk}`);
    
    audio.onended = () => {
      isPlayingRef.current = false;
      if (audioQueueRef.current.length > 0) {
        playAudioQueue();
      } else {
        setStatus('idle');
      }
    };

    try {
      await audio.play();
    } catch (err) {
      console.error('Audio playback failed:', err);
      isPlayingRef.current = false;
      // æ¸…ç©ºé˜Ÿåˆ—ï¼Œé˜²æ­¢å¡ä½
      audioQueueRef.current = [];
      setStatus('idle');
    }
  }, []);

  // å¼€å§‹å½•éŸ³
  const startRecording = useCallback(async () => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      alert(t.errors.backend_offline);
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
      mediaRecorderRef.current = mediaRecorder;

      // Reset send queue for new recording session
      sendQueueRef.current = Promise.resolve();

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          const blob = event.data;
          sendQueueRef.current = sendQueueRef.current.then(async () => {
            if (websocketRef.current?.readyState === WebSocket.OPEN) {
              try {
                const base64 = await blobToBase64(blob);
                websocketRef.current.send(JSON.stringify({ 
                  type: 'audio-chunk', 
                  content: base64 
                }));
              } catch (error) {
                console.error('Error processing audio chunk:', error);
              }
            }
          });
        }
      };

      mediaRecorder.onstop = () => {
        sendQueueRef.current = sendQueueRef.current.then(() => {
          if (websocketRef.current?.readyState === WebSocket.OPEN) {
            websocketRef.current.send(JSON.stringify({ type: 'audio-end' }));
          }
        });
      };

      mediaRecorder.start(200);
      setStatus('recording');
      
      setMessages(prev => [...prev, { role: 'user', text: t.welcome.user_placeholder }]);

    } catch (err) {
      console.error('Microphone access denied:', err);
      alert(t.errors.mic_access);
    }
  }, [t]);

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      
      setStatus('processing');
    }
  }, []);

  // å‘é€æ–‡æœ¬æ¶ˆæ¯
  const sendTextMessage = useCallback((text: string) => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      alert(t.errors.backend_offline);
      return;
    }

    const trimmedText = text.trim();
    if (!trimmedText) {
      return;
    }

    // å‘é€åˆ°åç«¯ï¼ˆåç«¯ä¼šå‘é€ user-message å›æ¥ï¼Œæ‰€ä»¥è¿™é‡Œä¸ç›´æ¥æ·»åŠ ï¼‰
    websocketRef.current.send(JSON.stringify({ 
      type: 'text-input', 
      content: trimmedText 
    }));
  }, [t]);

  return {
    messages,
    status,
    isConnected,
    startRecording,
    stopRecording,
    sendTextMessage
  };
};